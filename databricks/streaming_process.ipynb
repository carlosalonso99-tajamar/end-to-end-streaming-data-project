{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9c25d35-2c64-4eb9-a876-1464a9c0d795",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# PROCESAMIENTO DE LOS DATOS EN STREAMING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0347622-2b4e-41b5-8318-cdcad1b13209",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.- Conexión a EventHubs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c846ff2-98a2-4c0d-a819-521219fda0e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Lo primero que debemos hacer, es establecer la conexión a EventHubs para poder procesar los datos que nos llegan desde ahí, por lo que deberemos de proporcionar en el código tanto la cadena de conexión como el nombre del eventhubs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b0a84f5-572d-4c69-8926-3fe440dbc3a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "connectionString = \"<your_connection_string>\"\n",
    "eventHubName = \"<your_event_hub_name>\"\n",
    "\n",
    "ehConf = {\n",
    "  'eventhubs.connectionString' : sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connectionString),\n",
    "  'eventhubs.eventHubName': eventHubName\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9257373a-bc61-4ed8-8695-74b9b2e9f9ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Una vez establecida la conexión con EventHubs, tenemos que crear un dataframe que va a empezar a escuchar los eventos que recibe el servicio antes mencionado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1660ed25-cd7d-4c3a-aad4-bdfaad6763cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Reading stream: Load data from Azure Event Hub into DataFrame 'df' using the previously configured settings\n",
    "df = spark.readStream \\\n",
    "    .format(\"eventhubs\") \\\n",
    "    .options(**ehConf) \\\n",
    "    .load() \\\n",
    "\n",
    "# Displaying stream: Show the incoming streaming data for visualization and debugging purposes\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b145d167-3cc8-4c42-9931-a4b69b9e32ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.- Capas de información\n",
    "\n",
    "Para poder procesar los datos correctamente y tener todos los registros de ellos, vamos a crear 3 capas, que desempeñarán distintos roles:\n",
    "- Bronze layer: datos crudos (tal cual llegan del EventHubs)\n",
    "- Silver layer: datos limpios (con estructura definida)\n",
    "- Gold layer: datos definitivos (sacando nuevos valores, cribando los datos...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5052138-724e-4da0-becf-d64bfbd7559a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.1- Bronze layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35e7b0a4-0697-4a4e-ae4d-1db3c6bec6b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Para la Bronze layer vamos a crear una base de datos *streaming_bronze* para alojar allí los datos, y guardaremos la información que nos llega directa desde el EventHubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7be5302e-c215-49ab-80e1-5bd5a01cdd7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database 'streaming_bronze' created successfully or already exists.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Crear la base de datos/ esquema 'streaming_bronze' en el catálogo predeterminado\n",
    "    spark.sql(\"CREATE DATABASE IF NOT EXISTS streaming_bronze\")\n",
    "    print(\"Database 'streaming_bronze' created successfully or already exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error while creating database: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e54159d-e6b6-48c6-89cd-034a2b40c168",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7fba3fbf9bd0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.writeStream \\\n",
    "    .option(\"checkpointLocation\", \"/mnt/streaming/bronze_prueba/taxi_bronze\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .toTable(\"streaming_bronze.taxi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e68db5a4-0ddf-412e-acf3-115380f33227",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.2- Silver layer\n",
    "\n",
    "Para la creación de la silver layer, debemos crear el esquema que queremos para nuestros datos, y una vez definido deberemos limpiar el dataframe para escribir en la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1bd8c24-b240-4eb6-ab52-01f5dd91bc31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "json_schema = StructType([\n",
    "    StructField(\"car_id\", StringType(), True),\n",
    "    StructField(\"elevation\", DoubleType(), True),\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"distance_km\", DoubleType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e76a988-814b-44e9-a71c-a0816802b610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Crear la base de datos/ esquema 'streaming_bronze' en el catálogo predeterminado\n",
    "    spark.sql(\"CREATE DATABASE IF NOT EXISTS streaming_silver\")\n",
    "    print(\"Database 'streaming_silver' created successfully or already exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error while creating database: {e}\")\n",
    "\n",
    "df = spark.readStream\\\n",
    "    .format(\"delta\")\\\n",
    "    .table(\"streaming_bronze.taxi\")\\\n",
    "    .withColumn(\"body\", col(\"body\").cast(\"string\"))\\\n",
    "    .withColumn(\"body\",from_json(col(\"body\"), json_schema))\\\n",
    "    .select(\"body.*\")\n",
    "\n",
    "# Displaying stream: Visualize the transformed data in the DataFrame for verification and analysis\n",
    "df.display()\n",
    "\n",
    "# Writing stream: Save the transformed data to the 'streaming.silver.weather' Delta table in 'append' mode with checkpointing for data reliability\n",
    "df.writeStream\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/streaming/silver_prueba/taxi_silver\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .format(\"delta\")\\\n",
    "    .toTable(\"streaming_silver.taxi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bd4a4dd-0784-4c0f-b5b1-0dc29972bf51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.3- Gold Layer\n",
    "\n",
    "Para la capa gold, hemos incluido algunas transformaciones y hemos añadido ciertas columnas. Estos son nuestros requisitos:\n",
    "- Añadir el campo de **velocidad**: calculamos la velocidad en km/h según la distancia recorrida en un segundo\n",
    "- Añadir el campo de **consumo**: calculamos el consumo en cada tramo, estableciendo la constante del consumo del coche (en nuestro caso es 8 l/km), y teniendo en cuenta la distancia recorrida en cada tramo\n",
    "- Eliminar el campo elevation: decidimos esto ya que no tenía ninguna relevancia en nuestros datos\n",
    "- Filtrar velocidades: al ver que algunas velocidades eran irreales (datos malos), hemos filtrado el dataframe para cambiar el dato de velocidad, cuando este supera los 200 km/h, por un dato estándar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa3bcebe-d22d-410d-b589-b7a58152a620",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "try:\n",
    "    # Crear la base de datos/ esquema 'streaming_bronze' en el catálogo predeterminado\n",
    "    spark.sql(\"CREATE DATABASE IF NOT EXISTS streaming_gold\")\n",
    "    print(\"Database 'streaming_gold' created successfully or already exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error while creating database: {e}\")\n",
    "\n",
    "df = df.where(F.col(\"distance_km\") > 0)\n",
    "\n",
    "df = df.withColumn(\"speed_kmh\", F.round(df.distance_km * 3600, 2))\n",
    "\n",
    "df = df.withColumn(\"consumo\", F.round(df.distance_km * 8 / 100, 5))\n",
    "\n",
    "df = df.drop(\"elevation\")\n",
    "\n",
    "\n",
    "# Aplicar la función de ventana\n",
    "df = df.withColumn(\n",
    "    \"speed_kmh\",\n",
    "    F.when(\n",
    "        df.speed_kmh > 200,\n",
    "        F.lit(102.0)\n",
    "    ).otherwise(df.speed_kmh)\n",
    ")\n",
    "\n",
    "df.display()\n",
    "\n",
    "df.writeStream\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/streaming/gold_prueba1/taxi_gold\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .format(\"delta\")\\\n",
    "    .toTable(\"streaming_gold.taxi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f6e64d4-bfb2-4111-ac71-80af5f4778b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.- Envío de datos a Power Bi\n",
    "\n",
    "Finalmente, enviamos el dataframe final a un streaming dataset creado en Power Bi (en la nube), para poder hacer visualizaciones en tiempo real de nuestros datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af856115-c306-48c4-aa5e-ebff7d849305",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import json\n",
    "from pyspark.sql import functions as F\n",
    " \n",
    "# Endpoint de la API de Power BI para el streaming dataset\n",
    "url = f\"<your_powerbi_streaming_dataset_url>\"\n",
    " \n",
    "# Crear una función para enviar los datos a Power BI\n",
    "def send_to_powerbi(data):\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    if response.status_code == 200:\n",
    "        print(\"Datos enviados correctamente a Power BI.\")\n",
    "    else:\n",
    "        print(f\"Error al enviar los datos: {response.status_code}\")\n",
    " \n",
    "# Transformar el dataframe de Spark a una lista de diccionarios para enviar a Power BI\n",
    "def transform_and_send_to_powerbi(batch_df, batch_id):\n",
    "    # Convierte el dataframe a un formato adecuado para la API de Power BI\n",
    "    data = batch_df.select(\"car_id\",\"latitude\", \"longitude\", \"elevation\", \"timestamp\", \"distance_km\",\n",
    "                           \"speed_kmh\", \"consumo\").toPandas().to_dict(orient='records')\n",
    "   \n",
    "    # Enviar los datos a Power BI\n",
    "    send_to_powerbi(data)\n",
    " \n",
    "# Enviar los datos de manera continua usando writeStream\n",
    "df.writeStream \\\n",
    "    .foreachBatch(transform_and_send_to_powerbi) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    " \n",
    "df.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "streaming_process",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
